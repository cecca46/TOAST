{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105e87c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from spatial_OT.OT import *\n",
    "from spatial_OT.utils import *\n",
    "from scipy.spatial import distance\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "import ot\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "import warnings\n",
    "import time\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966af823",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "An Optimal-Transport based approach for quantifying the spatiotemporal growth of tissues.\n",
    "'''\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "def Cost(f, g, Grad, epsilon, device='cpu'):\n",
    "    '''\n",
    "    A matrix which is using for the broadcasted log-domain log-sum-exp trick-based updates.\n",
    "    ------Parameters------\n",
    "    f: torch.tensor (N1)\n",
    "        First dual variable of semi-unbalanced Sinkhorn\n",
    "    g: torch.tensor (N2)\n",
    "        Second dual variable of semi-unbalanced Sinkhorn\n",
    "    Grad: torch.tensor (N1 x N2)\n",
    "        A collection of terms in our gradient for the update\n",
    "    epsilon: float\n",
    "        Entropic regularization for Sinkhorn\n",
    "    device: 'str'\n",
    "        Device tensors placed on\n",
    "    '''\n",
    "    return -( Grad - torch.outer(f, torch.ones(Grad.size(dim=1), device=device)) - torch.outer(torch.ones(Grad.size(dim=0), device=device), g) ) / epsilon\n",
    "\n",
    "def normalize_M(M, C, N1, _ord='fro'):\n",
    "    '''\n",
    "    A function to return normalization constants for the gradient involving the merged feature-spatial matrix,\n",
    "    which is normalized by a factor related to the interslice feature cost C.\n",
    "    ------Parameters------\n",
    "    M: torch.tensor (N x N)\n",
    "        A symmetric matrix with positive entries (i.e. distance matrix, merged feature-spatial matrix)\n",
    "    C: torch.tensor (N x M) or (M x N)\n",
    "        Matrix of pairwise feature distances between slice 1 and 2\n",
    "    N1: int\n",
    "        Number of spots in first slice\n",
    "    '''\n",
    "    return (torch.linalg.norm(C, ord=_ord)**(1/2) / torch.linalg.norm(M, ord=_ord)) * (N1 )**(1/2)\n",
    "\n",
    "def LogSinkhorn_iteration(C, D1, D2, C1, C2, Pi_0=None, alpha=0.2, beta=0.5, gamma=50, epsilon=1e-1, max_iter=100, balanced=False, device='cpu', dtype=torch.float64, override_EDM=False, override_FDM=False):\n",
    "    '''\n",
    "    Sinkhorn algorithm for the balanced/partially unbalanced case, with log-domain stabilization.\n",
    "    ------Parameters------\n",
    "    C: torch.tensor (N1 x N2)\n",
    "        A matrix of pairwise feature distances between transcript vectors in slice 1 and slice 2 (interslice).\n",
    "    D1: torch.tensor (N1 x N1)\n",
    "        A matrix of pairwise Euclidean distances between points in slice 1.\n",
    "    D2: torch.tensor (N2 x N2)\n",
    "        A matrix of pairwise Euclidean distances between points in slice 2.\n",
    "    C1: torch.tensor (N1 x N1)\n",
    "        A matrix of pairwise feature distances between transcript vectors in slice 1 (intraslice).\n",
    "    C2: torch.tensor (N2 x N2)\n",
    "        A matrix of pairwise feature distances between transcript vectors in slice 2 (intraslice).\n",
    "    Pi_0: torch.tensor (N1 x N2)\n",
    "        An initialization for the alignment matrix Pi. Should respect marginals of semi-unbalanced or balanced.\n",
    "    alpha: float\n",
    "        A balance parameter between the interslice feature term of the objective and the merged feature-spatial term.\n",
    "    beta: float\n",
    "        A balance parameter between the GW (quartet) term and the triplet term of the merged feature-spatial term.\n",
    "    gamma: float\n",
    "        A hyperparameter controlling the strength of the KL-divergence term in the objective.\n",
    "    epsilon: float\n",
    "        A hyperparameter controlling the strength of the entropic regularization in the Sinkhorn algorithm.\n",
    "    max_iter: int\n",
    "        The maximal number of iterations DeST-OT is run.\n",
    "    balanced: bool\n",
    "        Boolean for whether to default to a balanced OT or to use DeST-OT's semi-unbalanced routine. Default set to False.\n",
    "    device: str\n",
    "        Device that torch tensors are placed on. Using GPU/'cuda' is much faster.\n",
    "    dtype: torch.type\n",
    "        The default datatype that the alignment and other tensors are in. Ideally torch.float64 or torch.float32\n",
    "    override_EDM: bool\n",
    "        Whether to override the merged feature-spatial matrix with a standard Euclidean distance matrix.\n",
    "    override_FDM:\n",
    "        Whether to override the merged feature-spatial matrix with an intraslice feature distance matrix.\n",
    "    '''\n",
    "    \n",
    "    N1, N2 = C.size(dim=0), C.size(dim=1)\n",
    "    \n",
    "    M1 = D1 * C1\n",
    "    M2 = D2 * C2\n",
    "    \n",
    "    if override_EDM:\n",
    "        # Override fused matrix with standard EDM\n",
    "        M1 = D1\n",
    "        M2 = D2\n",
    "    elif override_FDM:\n",
    "        # Override fused matrix with feature distance matrix\n",
    "        M1 = C1\n",
    "        M2 = C2\n",
    "    \n",
    "    # Normalizing constants\n",
    "    p1, p2 = normalize_M(M1, C, N1, _ord='fro'), \\\n",
    "                            normalize_M(M2, C, N1, _ord='fro')\n",
    "    r2, r1 = torch.linalg.norm(C, ord='fro')/torch.linalg.norm(M2**2, ord='fro') * (N1**1/2), \\\n",
    "                            torch.linalg.norm(C, ord='fro')/torch.linalg.norm(M1**2, ord='fro') * (N1 / N2**(1/2))\n",
    "    \n",
    "    k = 0\n",
    "    stationarity_gap = torch.inf\n",
    "    \n",
    "    # These are the same for this synthetic data-- change dimensions when we get there\n",
    "    one_N1 = torch.ones((N1), device=device, dtype=dtype)\n",
    "    one_N2 = torch.ones((N2), device=device, dtype=dtype)\n",
    "    \n",
    "    g1 = one_N1 / N1\n",
    "    \n",
    "    if balanced:\n",
    "        g2 = one_N2 / N2\n",
    "    else:\n",
    "        g2 = one_N2 / N1\n",
    "    \n",
    "    log_g1 = torch.log(g1)\n",
    "    log_g2 = torch.log(g2)\n",
    "    \n",
    "    if Pi_0 is None:\n",
    "        # Will converge to partially balanced marginal\n",
    "        Pi_0 = torch.outer(g1, g2).to(device)\n",
    "    \n",
    "    Pi_k = Pi_0\n",
    "    \n",
    "    f_k = torch.zeros((N1), device=device)\n",
    "    g_k = torch.zeros((N2), device=device)\n",
    "    \n",
    "    errs = []\n",
    "    \n",
    "    # unbalanced coefficient\n",
    "    ubc = gamma/(gamma + epsilon)\n",
    "    \n",
    "    grad = torch.inf\n",
    "    while k < max_iter:\n",
    "        \n",
    "        # Offering the user the option to choose their energy-regularization\n",
    "        dist_orig = r1*(M1**2 @ Pi_k) + r2*(Pi_k @ M2**2)\n",
    "        \n",
    "        if balanced:\n",
    "            dist_GW = -2 * ((p1*M1) @ Pi_k @ (p2*M2)) \n",
    "        else:\n",
    "            dist_GW = -2*((p1*M1) @ Pi_k @ (p2*M2).T) + (p1*M1)**2 @ Pi_k @ torch.ones((N2, N2), device=device, dtype=dtype)\n",
    "        \n",
    "        grad = (1-alpha)*C + alpha*(beta*dist_orig + (1-beta)*dist_GW)\n",
    "        \n",
    "        if balanced:\n",
    "            f_k = f_k + epsilon*(log_g1 - torch.logsumexp(Cost(f_k, g_k, grad, epsilon, device=device), axis=1))\n",
    "            g_k = g_k + epsilon*(log_g2 - torch.logsumexp(Cost(f_k, g_k, grad, epsilon, device=device), axis=0))\n",
    "        elif balanced is False:\n",
    "            # Partially unbalanced coefficient used on one of the dual variables\n",
    "            f_k = ubc*(f_k + epsilon*(log_g1 - torch.logsumexp(Cost(f_k, g_k, grad, epsilon, device=device), axis=1)) )\n",
    "            g_k = g_k + epsilon*(log_g2 - torch.logsumexp(Cost(f_k, g_k, grad, epsilon, device=device), axis=0))\n",
    "        \n",
    "        Pi = torch.exp(Cost(f_k, g_k, grad, epsilon, device=device))\n",
    "        # Checking whether we're approaching a fixed point\n",
    "        stationarity_gap = torch.linalg.norm(Pi_k - Pi, ord='fro')\n",
    "        Pi_k = Pi\n",
    "        \n",
    "        k+=1\n",
    "        errs.append(stationarity_gap)\n",
    "    \n",
    "    xi = (Pi_k @ one_N2 - g1)\n",
    "    \n",
    "    return xi, Pi_k, errs\n",
    "\n",
    "import torch\n",
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "from pynvml import *\n",
    "\n",
    "\n",
    "def intersect(lst1, lst2): \n",
    "    \"\"\"\n",
    "    param: lst1 - list\n",
    "    param: lst2 - list\n",
    "    \n",
    "    return: list of common elements\n",
    "    \"\"\"\n",
    "    temp = set(lst2)\n",
    "    lst3 = [value for value in lst1 if value in temp]\n",
    "    return lst3 \n",
    "\n",
    "\"\"\"\n",
    "This function to select a GPU is from the scSLAT package.\n",
    "\"\"\"\n",
    "def get_free_gpu() -> int:\n",
    "    \"\"\"\n",
    "    Get index of GPU with least memory usage\n",
    "    \n",
    "    Ref\n",
    "    ----------\n",
    "    https://stackoverflow.com/questions/58216000/get-total-amount-of-free-gpu-memory-and-available-using-pytorch\n",
    "    \"\"\"\n",
    "    nvmlInit()\n",
    "    index = 0\n",
    "    max = 0\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        h = nvmlDeviceGetHandleByIndex(i)\n",
    "        info = nvmlDeviceGetMemoryInfo(h)\n",
    "        index = i if info.free > max else index\n",
    "        max = info.free if info.free > max else max\n",
    "        \n",
    "    # seed = np.random.randint(1000)\n",
    "    # os.system(f'nvidia-smi -q -d Memory |grep Used > gpu-{str(seed)}.tmp')\n",
    "    # memory_available = [int(x.split()[2]) for x in open('gpu.tmp', 'r').readlines()]\n",
    "    # os.system(f'rm gpu-{str(seed)}.tmp')\n",
    "    # print(memory_available)\n",
    "    return index\n",
    "\n",
    "def xi_to_growth_rate(xi, t1=0, t2=1, normalize_xi=True):\n",
    "    '''\n",
    "    Returns a differential growth rate given the growth vector xi.\n",
    "    \n",
    "    Parameters:\n",
    "    xi: numpy array (N1)\n",
    "        Growth vector quantifying the raw mass-flux\n",
    "    t1: float\n",
    "        First observation timepoint\n",
    "    t2: float\n",
    "        Second observation timepoint\n",
    "    normalize_xi: bool\n",
    "        True if xi normalized to number of cells units (this is the default output of align, using the same flag \"normalize_xi\"),\n",
    "        False if xi directly computed from Pi without renormalization.\n",
    "    '''\n",
    "    N1 = xi.shape[0]\n",
    "    if normalize_xi is False:\n",
    "        Js = np.log(N1*xi + 1) / (t2 - t1)\n",
    "    else:\n",
    "        Js = np.log(xi + 1) / (t2 - t1)\n",
    "    # Returning a proper growth-rate given mass-flux xi\n",
    "    return Js\n",
    "\n",
    "def align(slice_t1, slice_t2, alpha=0.2, gamma=50, epsilon=1e-1, max_iter=100, balanced=False, use_gpu=True, normalize_xi=True, check_convergence=False):\n",
    "    \"\"\"\n",
    "    Run DeST-OT\n",
    "\n",
    "    Parameters:\n",
    "    slice_t1: AnnData object\n",
    "        The AnnData object of the first slice, with .obsm['spatial'] field storing the spatial coordinates\n",
    "    slice_t2: AnnData object\n",
    "        The AnnData object of the second slice, with .obsm['spatial'] field storing the spatial coordinates\n",
    "    alpha: float\n",
    "        A balance parameter between the interslice feature term of the objective and the merged feature-spatial term. Default is 0.2.\n",
    "    gamma: float\n",
    "        A hyperparameter controlling the strength of the KL-divergence term in the objective. Default is 50.\n",
    "    epsilon: float\n",
    "        A hyperparameter controlling the strength of the entropic regularization in the Sinkhorn algorithm. Default is 0.1.\n",
    "    max_iter: int\n",
    "        The maximal number of iterations DeST-OT is run. Default is 100.\n",
    "    balanced: bool\n",
    "        Boolean for whether to default to a balanced OT or to use DeST-OT's semi-unbalanced routine. Default set to False.\n",
    "    use_gpu: bool\n",
    "        Boolean for whether to use GPU. Default is True.\n",
    "    normalize_xi: bool\n",
    "        Boolean for whether to normalize the growth vector xi to the unit of number of spots. If True, each entry in xi will be in the unit of number of spots, e.g. xi_i = 1 means spots i will grow into two spots in the next timepoint. Default is False.\n",
    "        Note: set to False when use the output xi to compute the growth distortion metric.\n",
    "    check_convergence: bool\n",
    "        Boolean for whether to return the stationarity gap with respect to Pi for each iteration.\n",
    "\n",
    "    Returns:\n",
    "    Pi: numpy array of shape (N1 x N2)\n",
    "        The alignment matrix\n",
    "    xi: numpy array of shape (N1)\n",
    "        The growth vector\n",
    "    errs: list of size (max_iter)\n",
    "        If check_convergence is True, return a list of stationarity gaps for each iterations. Used for checking the convergence of Pi.\n",
    "    \"\"\"\n",
    "    X = slice_t1.X.toarray()\n",
    "    Y = slice_t2.X.toarray()\n",
    "    \n",
    "    C = distance.cdist(X, Y)\n",
    "    C1 = distance.cdist(X, X)\n",
    "    C2 = distance.cdist(Y, Y)\n",
    "    # Calculate spatial distances\n",
    "    D1 = distance.cdist(slice_t1.obsm['spatial'], slice_t1.obsm['spatial'])\n",
    "    D2 = distance.cdist(slice_t2.obsm['spatial'], slice_t2.obsm['spatial'])\n",
    "\n",
    "    # Pytorch\n",
    "    if use_gpu:\n",
    "        gpu_index = get_free_gpu()\n",
    "        device = torch.device(f'cuda:{gpu_index}' if torch.cuda.is_available() else 'cpu')\n",
    "    else:\n",
    "        device='cpu'\n",
    "    C = torch.from_numpy(C).to(device)\n",
    "    C1 = torch.from_numpy(C1).to(device)\n",
    "    C2 = torch.from_numpy(C2).to(device)\n",
    "    D1 = torch.from_numpy(D1).to(device)\n",
    "    D2 = torch.from_numpy(D2).to(device)\n",
    "\n",
    "    # Run DeST-OT\n",
    "    xi, Pi, errs = LogSinkhorn_iteration(C, D1, D2, C1, C2, alpha=alpha, gamma=gamma, epsilon=epsilon, max_iter=max_iter, balanced=balanced, device=device)\n",
    "    Pi = Pi.cpu().detach().numpy()\n",
    "    xi = xi.cpu().detach().numpy()\n",
    "\n",
    "    lseed=0\n",
    "    rseed=1\n",
    "    np.random.seed(lseed)\n",
    "    np.random.seed(rseed)\n",
    "    batch_size=1000\n",
    "    lidx = np.random.randint(0, X.shape[0], size=batch_size)\n",
    "    np.random.seed(rseed)\n",
    "    ridx = np.random.randint(0, Y.shape[0], size=batch_size)\n",
    "    C = distance.cdist(X[lidx], Y[ridx], metric=\"euclidean\")\n",
    "    Pi_sub = Pi[np.ix_(lidx, ridx)] \n",
    "    destot_dist = C * Pi_sub\n",
    "    sum_destot = np.sum(destot_dist)\n",
    "\n",
    "    if normalize_xi:\n",
    "        xi = slice_t1.shape[0] * xi\n",
    "    if check_convergence:\n",
    "        return Pi, xi, errs, \n",
    "    return Pi, xi, sum_destot\n",
    "\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "from scipy.spatial import distance\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def partial_procrustes_analysis(X, Y, pi):\n",
    "    m = np.sum(pi)\n",
    "    Z = (X - pi.sum(axis=1).dot(X) * (1.0 / m)).T\n",
    "    W = (Y - pi.sum(axis=0).dot(Y) * (1.0 / m)).T\n",
    "    H = W.dot(pi.T.dot(Z.T))\n",
    "    U, S, Vt = np.linalg.svd(H)\n",
    "    R = Vt.T.dot(U.T)\n",
    "    W = R.dot(W)\n",
    "    return Z.T, W.T\n",
    "\n",
    "\n",
    "def partial_stack_slices_pairwise(slices, pis):\n",
    "    assert len(slices) == len(pis) + 1, \"'slices' should have length one more than 'pis'. Please double check.\"\n",
    "    assert len(slices) > 1, \"You should have at least 2 layers.\"\n",
    "\n",
    "    new_coor = []\n",
    "    S1, S2 = partial_procrustes_analysis(slices[0].obsm['spatial'], slices[1].obsm['spatial'], pis[0])\n",
    "    new_coor.append(S1)\n",
    "    new_coor.append(S2)\n",
    "    for i in range(1, len(slices) - 1):\n",
    "        x, y = partial_procrustes_analysis(new_coor[i], slices[i + 1].obsm['spatial'], pis[i])\n",
    "        shift = new_coor[i][0,:] - x[0,:]\n",
    "        y = y + shift\n",
    "        new_coor.append(y)\n",
    "    new_slices = []\n",
    "    for i in range(len(slices)):\n",
    "        s = slices[i].copy()\n",
    "        s.obsm['spatial'] = new_coor[i]\n",
    "        new_slices.append(s)\n",
    "    return new_slices\n",
    "\n",
    "\n",
    "def transition_mat(Pi, celltypes1, celltypes2):\n",
    "    \"\"\"\n",
    "    Compute the cell type transition matrix from an alignment matrix Pi (Eq. 10 of the paper)\n",
    "\n",
    "    Parameters:\n",
    "    Pi: numpy array of shape (N1, N2)\n",
    "        The alignment matrix\n",
    "    celltype1: numpy array of shape N1\n",
    "        An array of celltypes for each spot on slice t1\n",
    "    celltype2: numpy array of shape N2\n",
    "        An array of celltypes for each spot on slice t2\n",
    "\n",
    "    Return:\n",
    "    The cell type transition matrix, a numpy array of shape (N_CT, N_CT), where N_CT is the total number of cell types\n",
    "    \"\"\"\n",
    "    celltypes_all = np.unique(np.concatenate((celltypes1,celltypes2), axis=0))\n",
    "    N_CT = celltypes_all.shape[0]\n",
    "    \n",
    "    T = np.zeros((N_CT, N_CT))\n",
    "    for i in range(N_CT):\n",
    "        for j in range(N_CT):\n",
    "            ct_i = celltypes_all[i]\n",
    "            ct_j = celltypes_all[j]\n",
    "            mask = np.outer((celltypes1 == ct_i), (celltypes2 == ct_j))\n",
    "            T[i,j] = np.sum(Pi[mask])\n",
    "    \n",
    "    col_sums = T.sum(axis=0)\n",
    "    non_zero_cols = col_sums != 0\n",
    "    T[:, non_zero_cols] /= col_sums[non_zero_cols]\n",
    "\n",
    "    return T\n",
    "\n",
    "\n",
    "def growth_distortion_metric_helper(xi, celltypes1, celltypes2, T=None):\n",
    "    '''\n",
    "    Implementation of the growth distortion metric given the inferred growth vector xi\n",
    "\n",
    "    Parameters:\n",
    "    xi: numpy array of shape (N1, N2)\n",
    "        The growth vector of a spatiotemporal alignment\n",
    "    celltype1: numpy array of shape N1\n",
    "        An array of celltypes for each spot on slice t1\n",
    "    celltype2: numpy array of shape N2\n",
    "        An array of celltypes for each spot on slice t2\n",
    "    T: numpy array of shape (N1, N2)\n",
    "        A cell type transition matrix to compute the growth distortion metric under.\n",
    "\n",
    "    Returns:\n",
    "    the growth distortion metric\n",
    "    '''\n",
    "    N1 = xi.shape[0]\n",
    "    if T is not None:\n",
    "        celltypes = np.unique(np.concatenate((celltypes1,celltypes2), axis=0))\n",
    "    else:\n",
    "        celltypes = np.intersect1d(celltypes1, celltypes2)\n",
    "    \n",
    "    distortion_measure = 0\n",
    "    \n",
    "    N_C = len(celltypes)\n",
    "    m_t = np.zeros(N_C)\n",
    "    m_tm1 = np.zeros(N_C)\n",
    "    \n",
    "    for p, celltype in np.ndenumerate(celltypes):\n",
    "        m_t[p] = np.sum(celltypes2 == celltype)\n",
    "        m_tm1[p] = np.sum(celltypes1 == celltype)\n",
    "    \n",
    "    if T is not None:\n",
    "        # Transition-matrix adjusted assignment of mass-fluxes\n",
    "        m_t = T @ m_t\n",
    "    \n",
    "    # Quantifying the accuracy of matching true baseline growth rates\n",
    "    growth_rates = np.zeros(celltypes.shape[0])\n",
    "    for p, celltype in np.ndenumerate(celltypes):\n",
    "        dmP = (m_t[p] - m_tm1[p])/N1\n",
    "        growth_rates[p] = gamma_tp = (1/N1)*((m_t[p] - m_tm1[p])/m_tm1[p])\n",
    "        xi_p = xi[celltypes1 == celltype]\n",
    "        # Sum of squares error of individual cell growth rates,\n",
    "        # relative to true cell type specific growth rate.\n",
    "        distortion_measure += np.sum( (xi_p - gamma_tp * np.ones(xi_p.shape[0]) )**2 )\n",
    "            \n",
    "    # print(f'Distortion metric value: {N1*distortion_measure}')\n",
    "    return N1 * distortion_measure\n",
    "\n",
    "\n",
    "def growth_distortion_metric(slice_t1, slice_t2, Pi, xi=None, annotation_key=\"annotation\", option=\"infer_transition\"):\n",
    "    \"\"\"\n",
    "    Compute the growth distortion metric given two slices, an alignment matrix Pi, and a growth vector xi\n",
    "\n",
    "    Parameters:\n",
    "    slice_t1: AnnData object\n",
    "        The AnnData object of the first slice, with .obs['annotation'] field storing the cell type of each spot.\n",
    "    slice_t2: AnnData object\n",
    "        The AnnData object of slice t2, with .obs['annotation'] field storing the cell type of each spot.\n",
    "    Pi: numpy array of shape (N1, N2)\n",
    "        Alignment matrix between slice t1 and slice t2\n",
    "    xi: numpy array of shape (N1) or NoneType\n",
    "        The growth vector from the alignment. If not input, then recomputed from Pi.\n",
    "    annotation_key: String\n",
    "        The key for the cell-type annotations in the AnnData object\n",
    "    option: String, one of the following two options\n",
    "        \"no_transition\": Assumes no cell type transition, and the growth distortion metric is calculcated based on the intersection of cell types in the two slices\n",
    "        \"infer_transition\": Assumes cell type transition during development, and the growth distortion metric is calculated based on the cell type transition matrix of all cell types that minimizes the growth distortion metric for the given Pi (Section 2.3.1 of the paper)\n",
    "\n",
    "    Returns:\n",
    "    The growth distortion metric of the given Pi and xi\n",
    "    \"\"\"\n",
    "    if xi is None:\n",
    "        one_N2 = np.ones(Pi.shape[1])\n",
    "        g1 = np.ones(Pi.shape[0])/Pi.shape[0]\n",
    "        xi = (Pi @ one_N2 - g1)\n",
    "    else:\n",
    "        pass\n",
    "    l1, l2 = slice_t1.obs[annotation_key].tolist(), slice_t2.obs[annotation_key].tolist()\n",
    "    l_merged = l1 + l2\n",
    "    categorical_labels, celltypes = pd.factorize(l_merged)\n",
    "    celltypes1 = categorical_labels[:len(l1)]\n",
    "    celltypes2 = categorical_labels[len(l1):]\n",
    "\n",
    "    if option == 'no_transition':\n",
    "        T = None\n",
    "    elif option == 'infer_transition':\n",
    "        T = transition_mat(Pi, celltypes1, celltypes2)\n",
    "    else:\n",
    "        raise Exception('Invalid Option')\n",
    "    \n",
    "    return growth_distortion_metric_helper(xi, celltypes1, celltypes2, T)\n",
    "\n",
    "\n",
    "def migration_metric(slice_t1, slice_t2, Pi):\n",
    "    \"\"\"\n",
    "    Compute the migration metric given two slices and an aligment matrix Pi\n",
    "\n",
    "    Parameters:\n",
    "    slice_t1: AnnData object\n",
    "        An AnnData object of slice t1, with .obsm['spatial'] field storing the spatial coordinates.\n",
    "    slice_t2: AnnData object\n",
    "        An AnnData object of slice t2, with .obsm['spatial'] field storing the spatial coordinates.\n",
    "    Pi: numpy array of shape (N1 x N2)\n",
    "        Alignment matrix between slice t1 and slice t2\n",
    "\n",
    "    Returns:\n",
    "    The migration metric of Pi\n",
    "    \"\"\"\n",
    "    slice_t1_newcoor_adata, slice_t2_newcoor_adata = partial_stack_slices_pairwise([slice_t1, slice_t2], [Pi])\n",
    "    \n",
    "    slice_t1_newcoor = slice_t1_newcoor_adata.obsm['spatial']\n",
    "    slice_t2_newcoor = slice_t2_newcoor_adata.obsm['spatial']\n",
    "    \n",
    "    V = distance.cdist(slice_t2_newcoor, slice_t1_newcoor)\n",
    "    \n",
    "    # Handling zero entries in Pi @ 1_n (i.e. spot not mapped at all to next timepoint)\n",
    "    ma = (Pi @ np.ones(Pi.shape[1])) > 0\n",
    "\n",
    "    Pi_hat = Pi[ma, :] / (Pi @ np.ones(Pi.shape[1]))[ma][:, None]\n",
    "    # Pi_hat = pi / (pi @ np.ones(pi.shape[1]))[:, None]\n",
    "    V = V[:,ma]\n",
    "\n",
    "    migs = np.einsum('ij,ji->i', Pi_hat, V)\n",
    "    \n",
    "    avg_weighted_migration_distance = np.mean(migs)\n",
    "    return avg_weighted_migration_distance\n",
    "\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "\n",
    "def compute_transported_adata_argmax(adata_source, adata_target, transport_matrix):\n",
    "    \"\"\"\n",
    "    Compute transported AnnData object using the argmax method.\n",
    "\n",
    "    Parameters:\n",
    "        adata_source (AnnData): Original AnnData before transport.\n",
    "        adata_target (AnnData): Target AnnData after transport.\n",
    "        transport_matrix (ndarray): Transport matrix (N_source x N_target).\n",
    "\n",
    "    Returns:\n",
    "        AnnData: New AnnData with updated spatial coordinates.\n",
    "    \"\"\"\n",
    "    # Extract spatial coordinates\n",
    "    spatial_target = adata_target.obsm[\"spatial\"]\n",
    "\n",
    "    # Find the target index with the highest probability for each source cell\n",
    "    max_indices = np.argmax(transport_matrix, axis=1)  # Shape: (N_source,)\n",
    "\n",
    "    # Assign new spatial positions from the most probable target cell\n",
    "    spatial_transported = spatial_target[max_indices]  # Shape: (N_source, 2)\n",
    "\n",
    "    # Create a new AnnData object for transported data\n",
    "    adata_transported = adata_source.copy()\n",
    "    adata_transported.obsm[\"spatial\"] = spatial_transported  # Update spatial positions\n",
    "\n",
    "    return adata_transported\n",
    "\n",
    "def compute_local_cell_type_distribution(adata, k=10):\n",
    "    \"\"\"\n",
    "    Compute the local cell-type distribution for each cell.\n",
    "\n",
    "    Parameters:\n",
    "        adata (AnnData): AnnData object with spatial coordinates and cell types.\n",
    "        k (int): Number of nearest neighbors to consider.\n",
    "\n",
    "    Returns:\n",
    "        ndarray: Local cell-type distributions (N_cells x N_cell_types).\n",
    "    \"\"\"\n",
    "    # Extract spatial coordinates and cell types\n",
    "    spatial_coords = adata.obsm[\"spatial\"]\n",
    "    cell_types = adata.obs[\"unsupervised_label\"].astype(\"category\").cat.codes.values  # Convert categorical to numerical\n",
    "    unique_types = np.unique(cell_types)  # Get unique cell type indices\n",
    "    num_types = len(unique_types)  # Number of unique cell types\n",
    "\n",
    "    # Compute k-nearest neighbors\n",
    "    knn = NearestNeighbors(n_neighbors=k).fit(spatial_coords)\n",
    "    neighbors = knn.kneighbors(spatial_coords, return_distance=False)\n",
    "\n",
    "    # Compute local cell-type distribution\n",
    "    local_distributions = np.zeros((spatial_coords.shape[0], num_types))\n",
    "\n",
    "    for i in range(spatial_coords.shape[0]):\n",
    "        neighbor_types = cell_types[neighbors[i]]  # Get types of neighbors\n",
    "        for j, cell_type in enumerate(unique_types):\n",
    "            local_distributions[i, j] = np.sum(neighbor_types == cell_type) / k  # Normalize\n",
    "\n",
    "    return local_distributions\n",
    "\n",
    "def compute_js_divergence_before_after(adata_source, adata_transported, k=5):\n",
    "    \"\"\"\n",
    "    Compute the average Jensen-Shannon (JS) divergence between the local cell-type distributions\n",
    "    before and after transport.\n",
    "\n",
    "    Parameters:\n",
    "        adata_source (AnnData): Original AnnData before transport.\n",
    "        adata_transported (AnnData): AnnData after transport.\n",
    "        k (int): Number of nearest neighbors to consider.\n",
    "\n",
    "    Returns:\n",
    "        float: Average JS divergence across all cells.\n",
    "    \"\"\"\n",
    "    # Compute local cell-type distributions\n",
    "    source_distributions = compute_local_cell_type_distribution(adata_source, k)\n",
    "    transported_distributions = compute_local_cell_type_distribution(adata_transported, k)\n",
    "\n",
    "    # Compute JS divergence for each cell\n",
    "    js_divergences = [\n",
    "        jensenshannon(source_distributions[i], transported_distributions[i])\n",
    "        for i in range(source_distributions.shape[0])\n",
    "    ]\n",
    "\n",
    "    # Return average JS divergence across all cells\n",
    "    return np.mean(js_divergences)\n",
    "\n",
    "\n",
    "def compute_accuracy_max_prob(transport_matrix, source_labels, target_labels):\n",
    "\n",
    "    source_labels = np.asarray(source_labels)\n",
    "    target_labels = np.asarray(target_labels)\n",
    "\n",
    "    # Get the target cell index with max probability for each source cell\n",
    "    max_prob_indices = np.argmax(transport_matrix, axis=1)\n",
    "    \n",
    "    # Predicted cell types based on max probability\n",
    "    predicted_labels = target_labels[max_prob_indices]\n",
    "    # Compare predicted labels with source labels\n",
    "    correct_predictions = (source_labels == predicted_labels)\n",
    "    \n",
    "    # Compute accuracy\n",
    "    accuracy = correct_predictions.mean()\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d9715d",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "alpha = 0.5  \n",
    "epsilon = 0.1  \n",
    "\n",
    "protein = sc.read_h5ad(\"/home/fceccarelli/home3/tmp/rna.h5ad\")\n",
    "sc.settings.set_figure_params(dpi_save=250)\n",
    "sc.pl.embedding(\n",
    "    protein,\n",
    "    basis=\"spatial\",\n",
    "    color=\"unsupervised_label\",\n",
    "    title=\"RNA leiden\",\n",
    "    s=0.8,\n",
    "    save=\"_protein_leiden.pdf\"   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dae9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rna = sc.read_h5ad(\"/home/fceccarelli/home3/tmp/protein.h5ad\")\n",
    "\n",
    "sc.pl.embedding(\n",
    "    rna,\n",
    "    basis=\"spatial\",\n",
    "    color=\"unsupervised_label\",\n",
    "    title=\"Protein leiden\",\n",
    "    s=0.8 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61e200c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rna.var_names == protein.var_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe219ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_true_matching(transport):\n",
    "    \"\"\"\n",
    "    Accuracy = fraction of RNA cells whose top transport match is the correct protein cell.\n",
    "    \"\"\"\n",
    "    pred = transport.argmax(axis=1)\n",
    "    true = np.arange(transport.shape[0])\n",
    "\n",
    "    return (pred == true).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690ca0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "results = []\n",
    "\n",
    "def compute_spatial_fgw_alignment(slice1, slice2, alpha, epsilon):\n",
    "    \"\"\"Compute FGW and FGW-SN alignment and return accuracy and JS divergence scores.\"\"\"\n",
    "   \n",
    "    X = slice1.X.toarray()\n",
    "    Y = slice2.X.toarray()\n",
    "    \n",
    "    # Compute spatial graphs\n",
    "    coords1 = pd.DataFrame(slice1.obsm[\"spatial\"], columns=[\"x\", \"y\"])\n",
    "    X_df = pd.DataFrame(X, columns=[f\"PC{i+1}\" for i in range(X.shape[1])])\n",
    "    \n",
    "    X_df[\"x\"], X_df[\"y\"], X_df[\"cell_type\"] = coords1[\"x\"].values, coords1[\"y\"].values, slice1.obs[\"unsupervised_label\"].values\n",
    "\n",
    "    G1 = build_knn_graph_from2d(X_df, k=k)\n",
    "    X_df[\"spatial_entropy\"] = X_df.index.map(compute_spatial_entropy(G1))\n",
    "    slice1_avg_expr = compute_average_neighbor_expression(G1, pd.DataFrame(X))\n",
    "    \n",
    "    coords2 = pd.DataFrame(slice2.obsm[\"spatial\"], columns=[\"x\", \"y\"])\n",
    "    Y_df = pd.DataFrame(Y, columns=[f\"PC{i+1}\" for i in range(Y.shape[1])])\n",
    "    Y_df[\"x\"], Y_df[\"y\"], Y_df[\"cell_type\"] = coords2[\"x\"].values, coords2[\"y\"].values, slice2.obs[\"unsupervised_label\"].values\n",
    "\n",
    "    G2 = build_knn_graph_from2d(Y_df, k=k)\n",
    "    Y_df[\"spatial_entropy\"] = Y_df.index.map(compute_spatial_entropy(G2))\n",
    "    slice2_avg_expr = compute_average_neighbor_expression(G2, pd.DataFrame(Y))\n",
    "    \n",
    "    # Compute cost matrices\n",
    "    M = distance.cdist(X, Y).astype(float)\n",
    "    C1 = distance.cdist(slice1.obsm[\"spatial\"], slice1.obsm[\"spatial\"]).astype(float)\n",
    "    C2 = distance.cdist(slice2.obsm[\"spatial\"], slice2.obsm[\"spatial\"]).astype(float)\n",
    "    C3 = np.abs(X_df[\"spatial_entropy\"].values[:, np.newaxis] - Y_df[\"spatial_entropy\"].values[np.newaxis, :])\n",
    "    C4 = distance.cdist(slice1_avg_expr.values, slice2_avg_expr.values).astype(float)\n",
    "\n",
    "    # Normalize matrices\n",
    "    for mat in [M, C1, C2, C3, C4]:\n",
    "        mat /= mat.max() if mat.max() > 0 else 1  # Avoid division by zero\n",
    "    \n",
    "    # Compute transport maps\n",
    "    p, q = ot.unif(X.shape[0]), ot.unif(Y.shape[0])\n",
    "    G0 = np.outer(p, q)\n",
    "\n",
    "    FGW = compute_transport(G0, epsilon, alpha, C1, C2, p, q, M)\n",
    "    FGW_SN = compute_transport(G0, epsilon, alpha, C1, C2, p, q, M, C3, C4)\n",
    "\n",
    "    acc_fgw = accuracy_true_matching(FGW)\n",
    "    acc_fgw_sn = accuracy_true_matching(FGW_SN)\n",
    "\n",
    "    acc_fgw_cluster = compute_accuracy_max_prob(FGW, slice1.obs['unsupervised_label'], slice2.obs['unsupervised_label'])\n",
    "    acc_fgw_sn_cluster = compute_accuracy_max_prob(FGW_SN, slice1.obs['unsupervised_label'], slice2.obs['unsupervised_label'])\n",
    "    \n",
    "    # Compute JS divergence\n",
    "    js_fgw = compute_js_divergence_before_after(slice1, compute_transported_adata_argmax(slice1, slice2, FGW), k=20)\n",
    "    js_fgw_sn = compute_js_divergence_before_after(slice1, compute_transported_adata_argmax(slice1, slice2, FGW_SN), k=20)\n",
    "\n",
    "    return acc_fgw, acc_fgw_sn, acc_fgw_cluster, acc_fgw_sn_cluster, js_fgw, js_fgw_sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f04b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.5  \n",
    "epsilon = 0.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ced549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_grid(adata, grid_size=100):\n",
    "    x = adata.obsm[\"spatial\"][:, 0]\n",
    "    y = adata.obsm[\"spatial\"][:, 1]\n",
    "\n",
    "    # Compute grid indices\n",
    "    grid_x = (x // grid_size).astype(int)\n",
    "    grid_y = (y // grid_size).astype(int)\n",
    "\n",
    "    # Convert to Python strings BEFORE concatenating\n",
    "    grid_x_str = grid_x.astype(str).astype(object)\n",
    "    grid_y_str = grid_y.astype(str).astype(object)\n",
    "\n",
    "    adata.obs[\"grid_id\"] = grid_x_str + \"_\" + grid_y_str\n",
    "\n",
    "    return adata\n",
    "\n",
    "rna = assign_grid(rna, grid_size=100)\n",
    "protein = assign_grid(protein, grid_size=100)\n",
    "\n",
    "rna_bins = set(rna.obs[\"grid_id\"])\n",
    "prot_bins = set(protein.obs[\"grid_id\"])\n",
    "\n",
    "common_bins = list(rna_bins.intersection(prot_bins))\n",
    "print(\"Common grids:\", len(common_bins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd92302",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "transport_list_fgw = []\n",
    "transport_list_fgw_sn = []\n",
    "\n",
    "\n",
    "for i, g in enumerate(common_bins):\n",
    "    print(f\"Running alignment on grid {i}\")\n",
    "\n",
    "    rna_g = rna[rna.obs[\"grid_id\"] == g].copy()\n",
    "    prot_g = protein[protein.obs[\"grid_id\"] == g].copy()\n",
    "\n",
    "    # Skip tiny grids\n",
    "    if rna_g.n_obs < 20 or prot_g.n_obs < 20:\n",
    "        continue\n",
    "\n",
    "    acc_fgw, acc_fgw_sn, acc_fgw_cluster, acc_fgw_sn_cluster, js_fgw, js_fgw_sn = \\\n",
    "        compute_spatial_fgw_alignment(rna_g, prot_g, alpha, epsilon)\n",
    "\n",
    "    Pi, _, _ = align(rna_g, prot_g, alpha=0.2, gamma=50, epsilon=0.1, max_iter=100, balanced=False, use_gpu=False, normalize_xi=True, check_convergence=False)\n",
    "    acc_dest = accuracy_true_matching(Pi)\n",
    "    \n",
    "    acc_dest_cluster = compute_accuracy_max_prob(Pi, rna_g.obs['unsupervised_label'], prot_g.obs['unsupervised_label'])\n",
    "    js_dest = compute_js_divergence_before_after(rna_g, compute_transported_adata_argmax(rna_g, prot_g, Pi), k=20)\n",
    "    \n",
    "    # Save results\n",
    "    results.append({\n",
    "        \"grid\": g,\n",
    "        \"ACC_FGW\": acc_fgw,\n",
    "        \"ACC_TOAST\": acc_fgw_sn,\n",
    "        \"ACC_DEST\": acc_dest,\n",
    "        \"ACC_FGW_CLUSTER\": acc_fgw_cluster,\n",
    "        \"ACC_TOAST_CLUSTER\": acc_fgw_sn_cluster,\n",
    "        \"ACC_DEST_CLUSTER\": acc_dest_cluster,\n",
    "        \"JS_FGW\": js_fgw,\n",
    "        \"JS_TOAST\": js_fgw_sn,\n",
    "        \"JS_DEST\": js_dest,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3115d483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "print(\"Mean Acc FGW:\", df[\"ACC_FGW\"].mean())\n",
    "print(\"Mean Acc TOAST:\", df[\"ACC_TOAST\"].mean())\n",
    "print(\"Mean Acc DeST-OT:\", df[\"ACC_DEST\"].mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "visium",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
